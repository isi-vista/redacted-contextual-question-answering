% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{achiam2023gpt,
  title = {{GPT}-4 technical report},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  year = 2023,
  doi = {10.48550/arXiv.2303.08774},
  eprint = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}

@misc{almazrouei2023falcon,
  title = {The {F}alcon Series of Open Language Models},
  author = {Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
  year = 2023,
  doi = {10.48550/arXiv.2311.16867},
  eprint = {2311.16867},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}

@misc{evertz2024whispers,
  title = {Whispers in the Machine: Confidentiality in {LLM}-integrated Systems},
  author = {Jonathan Evertz and Merlin Chlosta and Lea Schönherr and Thorsten Eisenhofer},
  year = 2024,
  doi = {10.48550/arXiv.2402.06922},
  eprint = {2402.06922},
  archiveprefix = {arXiv},
  primaryclass = {cs.CR},
}

@inproceedings{fei-etal-2023-mitigating,
  title = {Mitigating Label Biases for In-context Learning},
  author = {Fei, Yu  and Hou, Yifan  and Chen, Zeming  and Bosselut, Antoine},
  year = 2023,
  month = jul,
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  pages = {14014--14031},
  doi = {10.18653/v1/2023.acl-long.783},
  url = {https://aclanthology.org/2023.acl-long.783},
  editor = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
  abstract = {Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model{'}s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model{'}s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37{\%} in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.},
}

@misc{gallegos2024bias,
  title = {Bias and Fairness in Large Language Models: A Survey},
  author = {Isabel O. Gallegos and Ryan A. Rossi and Joe Barrow and Md Mehrab Tanjim and Sungchul Kim and Franck Dernoncourt and Tong Yu and Ruiyi Zhang and Nesreen K. Ahmed},
  year = 2024,
  doi = {10.48550/arXiv.2309.00770},
  eprint = {2309.00770},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}

@misc{jiang2023mistral,
  title = {Mistral {7B}},
  author = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year = 2023,
  doi = {10.48550/arXiv.2310.06825},
  eprint = {2310.06825},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}

@article{lewis2020retrieval,
  title = {Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  year = 2020,
  journal = {Advances in Neural Information Processing Systems},
  volume = 33,
  pages = {9459--9474},
  url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
}

@inproceedings{meade-etal-2023-using,
  title = {Using In-Context Learning to Improve Dialogue Safety},
  author = {Meade, Nicholas  and Gella, Spandana  and Hazarika, Devamanyu  and Gupta, Prakhar  and Jin, Di  and Reddy, Siva  and Liu, Yang  and Hakkani-Tur, Dilek},
  year = 2023,
  month = dec,
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  pages = {11882--11910},
  doi = {10.18653/v1/2023.findings-emnlp.796},
  url = {https://aclanthology.org/2023.findings-emnlp.796},
  editor = {Bouamor, Houda  and Pino, Juan  and Bali, Kalika},
  abstract = {While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, often perpetuating social biases or stereotypes. We investigate a retrieval-based approach for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with existing approaches to dialogue safety without requiring training. We also show, using automatic and human evaluation, that reductions in toxicity obtained using our approach are not at the cost engagingness or coherency. Finally, we note our method can be used in compliment to existing dialogue safety approaches, such as RLHF.},
}

@inproceedings{oba-etal-2024-contextual,
  title = {In-Contextual Gender Bias Suppression for Large Language Models},
  author = {Oba, Daisuke  and Kaneko, Masahiro  and Bollegala, Danushka},
  year = 2024,
  month = mar,
  booktitle = {Findings of the Association for Computational Linguistics: EACL 2024},
  publisher = {Association for Computational Linguistics},
  address = {St. Julian{'}s, Malta},
  pages = {1722--1742},
  url = {https://aclanthology.org/2024.findings-eacl.121},
  editor = {Graham, Yvette  and Purver, Matthew},
  abstract = {Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.},
}

@inproceedings{rao-etal-2023-ethical,
  title = {Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in {LLM}s},
  author = {Rao, Abhinav  and Khandelwal, Aditi  and Tanmay, Kumar  and Agarwal, Utkarsh  and Choudhury, Monojit},
  year = 2023,
  month = dec,
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  pages = {13370--13388},
  doi = {10.18653/v1/2023.findings-emnlp.892},
  url = {https://aclanthology.org/2023.findings-emnlp.892},
  editor = {Bouamor, Houda  and Pino, Juan  and Bali, Kalika},
  abstract = {In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.},
}

@misc{rollings2023secretkeeping,
  title = {Secret-Keeping in Question Answering},
  author = {Nathaniel W. Rollings and Kent O'Sullivan and Sakshum Kulshrestha},
  year = 2023,
  journal = {arXiv preprint arXiv:2303.09067},
  doi = {10.48550/arXiv.2303.09067},
  eprint = {2303.09067},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}

% Default export had this as "Team, Gemma" which is... not right :-)
@article{team2024gemma,
  title = {Gemma: Open models based on {G}emini research and technology},
  author = {Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and Gemma Team and others},
  year = 2024,
  journal = {arXiv preprint arXiv:2403.08295},
  doi = {10.48550/arXiv.2403.08295},
  eprint = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}
